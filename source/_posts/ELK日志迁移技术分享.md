title: ELK日志迁移技术分享
author: strongant
tags: 'elk,elasticsearch'
date: 2020-09-14 21:03:08
---

# 日志处理方案演进



## 1.1 什么是日志

  日志是带时间戳的基于时间序列的机器数据，包括IT系统信息（服务器、网络设备、操作系统、应 用软件）、物联网各种传感器信息。日志反映用户行为，是事实数据，也是系统运维、故障诊断、性能分析的重要来源。对于任何系统，日志都是极其重要的组成部分。



## 1.2 日志处理的背景

  随着大数据时代的来临，系统日志量也呈指数级增加。随着日志格式复杂度的增加、日志规模的扩大以及应用节点的增多，传统的日志量过大分析耗时耗力、效率低下、无法胜任复杂的统计分析等。并且传统的日志分析无法满足实时化分析的业务需求，实时化分析也是当今大数据技术的发展趋势之一。



## 1.3 日志处理方案演进

  业界一般对日志的处理方案演进如下：



* 日志处理古老版

  日志没有集中式处理，一般情况下手动到服务器上使用 Linux 命令排查日志；日志的唯一作用就是做故障之后的排查；使用数据库存储日志，无法胜任复杂的实时统计分析需求。

* 日志处理离线版

  一般使用 Hadoop 平台实现日志的离线批量处理，缺点便是实时性太差；

  也有使用 Storm 流处理框架、Spark 内存计算框架处理日志， Hadoop/Storm/Spark 都是编程框架，并不是拿来就可以使用的平台。

* 日志处理实时版

  一般使用日志实时搜索引擎分析日志，其作为代表的解决方案主要有 Splunk(Splunk是一个分析计算机系统产生的机器数据，并在广泛的场景中提供数据收集、分析、可视化分布式的数据计算平台。)、ELK、EFK（EFK由ElasticSearch、Fluentd和Kiabana三个开源工具组成。Fluentd也是一个实时开源的数据收集器,）。



优点有什么？

1. 快速，日志从产生到搜索分析出结果只有数秒延时；
2. 可以处理大量数据，每天可以处理 TB 级日志量；
3. 很灵活，可以分析任何的实时日志。



其中，ELK 在业界最受欢迎，目前我们也使用的是ELK日志处理方案。ELK 是 Elastic 公司出品的开源实时日志处理与分析的解决方案，ELK 分别代表分布式搜索引擎 Elasticsearch、日志采集与解析工具 Logstash、日志可视化分析工具 Kibana，ELK日志处理方案具有配置灵活、集群可线性扩展、日志实时导入、检索性能高效、可视化分析方便等优点，已经成为业界日志处理方案的不二选择。



## ELK 架构解读



## 2.1 架构选择

  完整的日志分析系统主要包括日志采集系统、日志解析系统、日志存储系统和可视化分析系统四部分组成。典型的 ELK 架构图如下所示：


![](http://assets.processon.com/chart_image/5ca22e87e4b029f6dae31f31.png)

ELK 架构中，由于 Logstash 既作为日志搜集器又作为日志解析器，本身会消耗服务器较多的CPU 和 内存，如果服务器计算资源不够丰富，则会造成服务器性能下降无法正常工作。为了解决Logstash 占用系统资源高的问题，Elastic 公司推出了轻量级的日志采集器 Beats，在数据收集方面取代 Logstash，引入 Beats 后的系统架构如如下：



![](http://assets.processon.com/chart_image/5ca2317ee4b035b243be603c.png)

Beats 是一系列采集器的总称，对于不同的日志源和日志格式可以使用不同的Beats，截止目前为止，Beats 家族包括以下 7 个成员：



- Filebeat：轻量级的日志采集器，可用于收集文件数据。
- Metricbeat：5.0版本之前名为Topbeat，收集系统、进程和文件系统级别的 CPU 和内存使用情况等数据。
- Packetbeat：收集网络流数据，可以实时监控系统应用和服务，可以将延迟时间、错误、响应时间、SLA性能等信息发送到Logstash或Elasticsearch。
- Winlogbeat：Windows 事件日志
- Heartbeat：运行时间监控。
- Auditbeat: 审计日志数据采集。
- Functionbeat：无需服务器的采集器。



  目前对于日志规模较大的场景，还需要引入消息队列，Logstash 支持常用的消息队列有 Kafka、RabbitMQ、Redis等。引入消息队列后的架构如下：



![](http://assets.processon.com/chart_image/5ca236dee4b034408de95f19.png)

## 2.2 日志采集模块分析

  日志采集是对日志进行分析的基础，我们对于不同的日志类型可以选择不同类型的 beats。在我们的日常使用中，基本大多数情况以文件类型的日志居多，这里我们就以filebeat 采集文件类型的日志进行探索日志采集的原理。



  FileBeat 是采用Go 语言由 Elastic 官方开发的轻量级日志采集器，在应用服务器上以代理的形式安装。当这个 FileBeat 进行启动时，它会启动一个 prospector 监控日志路径或者日志文件，每一个日志文件都会有一个对应的 harvester， harvester 按行读取日志内容并转发至后台程序。FileBeat 维护了一个记录文件读取信息的注册文件，记录每个 harvester 最后读取位置的偏移量。日志按行读取以后，转发至 logstash 做解析。



### 2.3 日志解析模块分析

  Logstash是分布式数据收集引擎，在日志处理中担任搬运工的角色，它支持动态的从各种数据源搜集数据，并对数据进行过滤、分析、统一格式等操作，然后输出到用户指定的位置。事件流通过Logstash中的Input、Filter和Output插件处理和转换，Input用于配置日志的输入源，Filter用来解析日志，Output指定日志的输出去向。

  Grok是Logstash的一个正则解析插件，它能对日志流进行解析，内置了120多种的正则表达式库，对日志解析时需要针对日志格式定义相应的正则表达式。结合Grok提供的便利，总结出按行读取文件格式日志的解析步骤：

（1）首先对日志进行分析，明确每一个字段的含义，确定日志的切分规则，也就是一条日志切分成哪几个字段，这些字段是以后做日志分析的关键，对原始日志的解读和分析是非常关键的一步。

（2）根据步骤（1）的切分原则确定提取每一个字段的正则表达式，如果Grok中的正则库满足需求直接使用即可，否则采用自定义模式，两者可以组合使用。

（3）在Grok Debugger调试工具中调试、验证解析日志的正则表达式是否正确。

举一个例子，下面是一条半结构化日志：

```
2017-12-16 00:00:01,570 133382978 [  HandleConsumer.java:445:INFO ]  车辆号牌为空
```

对应的Grok表达式如下：

```
%{TIMESTAMP_ISO8601:time}\s*%{NUMBER:bytes}\s*\[\s*%{JAVAFILE:class}\:%{NUMBER:lineNumber}\s*\:%{LOGLEVEL:level}\s*\]\s*(?<info>([\s\S]*))
```

格式化后的日志如下：

```
{
  "time": "2017-12-16 00:00:01,570",
  "bytes":"133382978",
  "class": "HandleConsumer.java",
  "lineNumber": "445",
  "level": "INFO",
  "info":"车辆号牌为空。"
}
```

Grok Debugger调试正则表达式如图4所示：

![enter image description here](http://images.gitbook.cn/f58fff30-fc69-11e7-b269-5929bdce6d95)



### 2.4 日志存储模块分析

  Elasticsearch是存储日志的中央系统，实际项目中，根据需求搭建Elasticsearch集群即可。



### 2.5 日志可视化模块分析

  Kibana是一个开源日志分析及可视化平台，使用它可以对存储在Elasticsearch索引中的数据进行高效的搜索、可视化、分析等各种操作。Kibana的愿景是让海量数据更容易理解，通过浏览器的用户界面可以创建各种高级图表进行数据分析和展示，它的仪表盘功能可以汇总多个单操作图表，并且可以实时显示查询动态。

